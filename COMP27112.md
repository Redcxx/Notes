> COMP27112 Image Processing Revision Notes 2019-2020
>
> Author: weilue luo
>
> Note that this note is not exhaustive and contains personal understanding

# Glossary

#### Primitives

basic elements of graphics input (point, line, text, circle...)

#### [Fragments](https://en.wikipedia.org/wiki/Fragment_(computer_graphics))

The data needed to generate a single pixel's worth of drawing primitive in the data buffer *(a portion in RAM that drives video display)*. These data includes raster positions, depth, interpolated attributes, alpha, window ID... A fragment *maybe discarded* if it is farther away from a another pixels (according to depth buffer), a fragment *can replace another pixel* if it is nearer than what is already there and it can also be replace with *mixture of fragment's color and existing pixel's color* if alpha blending is used.

#### [Pixels](https://en.wikipedia.org/wiki/Pixel)

Pixel is the smallest controllable unit in a raster image. Each pixel is a ***sample*** of the original image, the more samples we have, the more accurate presentation of the original image.

#### [Raster Image](https://en.wikipedia.org/wiki/Raster_graphics) (栅格)

It is a dot matrix data structure to represent a (usually) rectangular grid of pixels.

#### Rendering

A process which a computer creates an image from model.

# OpenGL

It is an API for doing 3D computer graphics. It stands for Open Graphics Library, it is designed to be cross platform, language independent and extensible. Origins in Silicon Graphics Inc and first release in 1992, now run by **Khronos Group** *(This name is tested in quiz)* and is a open standard for portable graphics programming. Its members include many companies but not Microsoft --- they have **DirectX**.

OpenGL is rasterization-based and implemented as a client-server system.

- Point: regular (square) and anti-aliased (circle)
- Lines: consists of geometry (shape) and attributes (appearance)
- Polygons: quadrilaterals, triangle and convex polygons
- Transformations
- Camera Model --- 2D view of 3D scene
- Hidden surface removal
- lighting
  - local methods: flat, gouraud and phong (this gives shining which often what we want)
- Modelling & Rendering: model + shading + texture
- Blending & Transparency
- Support libraries
  - GLU: wrap low level details such as curves and surfaces and provides utilities such as viewing, texturing and tessellation
  - GLUT: interactions such as simple menu system and primitives such as cone and sphere

**Note** that OpenGL uses 4 x 4 matrix to encode 3D transformation.

### View with camera (5.2)

There are three steps to set up the camera view in OpenGL:

1. Use `gluLookAt()` to specify the camera, target and upright position.
2. Use `glortho()` or `glperspective()` to specify view type.
3. Use `glViewport()` to specify size and shape of the display (optional), default is to use the whole window. 

The camera model above is always active and implemented using **transformation matrix**, transformation matrix consists of two matrix: modelview matrix $M$ and projection matrix $P$. The modelview matrix composes the scene in the world and then use camera to take a view of it (step one above), then the projection matrix is applied to the camera (step two). So for each pixel $p$, it will be transformed into a new position $p' = P * M * p$. Note that modelview matrix will be applied before projection matrix.

Initially, OpenGL set both matrix as identity matrix and let the programmer to freely edit the values. Normally, we adjust the modelview matrix in the `display()` function and projection view matrix in the `reshape()` function.

#### Orthographic (Parallel) & Perspective View

| ![ortho & perspective image](https://i.stack.imgur.com/zyGF1.gif) |
| :----------------------------------------------------------: |
| *left perspective & right orthographic* [source](https://blender.stackexchange.com/a/649) |

Orthographic view can provide an accurate view, useful for things such as map; Perspective view is how we see the world.

# Crucial Concepts

> #### Lecture 2

#### 3D Cartesian Coordinates

3D Space in term of $xyz$.

#### Vector

It represents a direction in the space. A vector with distance one in that direction is called the unit vector.

#### Coordinates and Vectors

Both can be represent using $xyz$. Represent a point in the space and the spatial relationship between the origin and the point respectively.

#### Rotation

##### 2D

Let say you have a point in the 2D space represented by a vector of length $R$ and it has an angle of $\theta^{\circ}$ from the x-axis. By trigonometry we can calculate the point $xy$ by $x=R\cos(\theta),y=R\sin(\theta)$.

If we want to rotate this point about the origin by angle $\phi^{\circ}$, then we can simply calculate:
$$
\begin{align*}
x'&=R\cos(\theta+\phi)\\
&=R\cos(\theta)\cos(\phi)-R\sin(\theta)\sin(\phi)\\
&=x\cos(\phi)-y\sin(\phi)\\
y'&=R\cos(\theta+\phi)\\
&=R\sin(\theta)\cos(\phi)+R\cos(\theta)\sin(\phi)\\
&=x\sin(\phi)+y\cos(\phi)
\end{align*}
$$
By rewriting computation of new $x$ and new $y$ into sum of coefficient of $x$ and $y$, we can use a matrix (multiplication) to represent rotation.
$$
\begin{pmatrix}
\cos(\phi) & -\sin(\phi)\\
\sin(\phi) & \cos(\phi)
\end{pmatrix}
*
\begin{pmatrix}
x\\
y
\end{pmatrix}
=
\begin{pmatrix}
x'\\
y'
\end{pmatrix}
$$

##### 3D

Rotation in 3D space is kind of similar, for rotating around z-axis:
$$
\begin{pmatrix}
\cos(\phi) & -\sin(\phi) & 0\\
\sin(\phi) & \cos(\phi) & 0\\
0&0&1
\end{pmatrix}
*
\begin{pmatrix}
x\\
y\\
z
\end{pmatrix}
=
\begin{pmatrix}
x'\\
y'\\
z
\end{pmatrix}
$$

##### Rotation around arbitrary stuff

If we want to rotate a point $P$ around any point $O$ rather than just origin, then we can just translate $O$ to the origin, perform rotation and translate back to original $O$. We do similar stuff when we want to perform scaling.

For rotation about an axis, we need to move the point's vector to match one of the axis and rotate around that axis, then we translate the vector back to the original place.

![rotation about 3D axis image](https://i.ibb.co/56rkD4m/screenshot-online-manchester-ac-uk-2020-06-03-16-50-36.png)

#### Translation

So what about translation? Can we perform translation on a arbitrary point? It turns out that without information of the old $xyz$ we cannot encode the same translation for every $xyz$ in the space. So what do we do? We add a new dimension called **Homogenous Coordinates** to encode the translation:
$$
\begin{pmatrix}
1 & 0 & 0 & tx\\
0 & 1 & 0 & ty\\
0 & 0 & 1 & tz\\
0 & 0 & 0 & 1
\end{pmatrix}
*
\begin{pmatrix}
x\\
y\\
z\\
w
\end{pmatrix}
=
\begin{pmatrix}
x'\\
y'\\
z'\\
w
\end{pmatrix}
$$
This way, we can encode the same translation for any $xyz$.

#### Composing Transformation

Basically, $T_1 * T_2 * M = (T_1 * T_2) * M = T_{composite} * M$.

#### Non-commutativity

Generally, $M_1 * M_2 \neq M_2 * M_1$. Unless both of them are 1D (Vector).

#### Matrix Inverse

Two matrices $A$ and $B$ are inverse of each other if $A * B = I$ where $I$ is the identity matrix. Given $A$ there are algorithms to compute $B$ but not all matrix has inverse (singular matrix). For example:
$$
\begin{pmatrix}
1&0&0&0\\
0&0&0&0\\
0&0&1&0\\
0&0&0&1\\
\end{pmatrix}
$$
This matrix transformation truncate y value to zero, there is no way to recover the original number.

> #### Lecture 3

#### Graphics Building Blocks

We usually build graphics on computer with polygons and usually triangles. This is because meshes of polygons can model many different types of 3D objects well.

Some polygons come in funny shapes, OpenGL do not like it (probably due to implementation convenient) , it prefers convex polygon (all angles <= 180).

![funny polygons](https://i.ibb.co/Kxvck3d/image.png)

##### Tessellation

When you have a complex polygon, you can break it down using tessellation (provided in GLU).

![tessellation](https://i.ibb.co/yStLHW6/image.png)

##### Surface Normal

The surface normal is a vector perpendicular to the plane of the polygon, this turns out to be very useful, it can be used to describe orientation, lightning calculations, collusion, culling...

The **cross product** of two vectors result in a vector perpendicular to both of them (length determine by the angle between two vectors).

##### Representing Scene

We can keep track of a huge list of polygons called **polygon soup** and paint them to the screen, but a scene often consists of surface which has shared vertices, polygon soup approach destroy this semantics (hard to interact since we do not know which surface a polygon belong) and waste many space (duplicating vertices).

Therefore we often use **polygon meshes** (a linked group of polygons), for example triangle strips:

<img src="https://i.ibb.co/qMZRHPr/image.png" alt="triangle strips"  />

This way we can represent $N$ triangles with $N+2$ vertices rather than $3N$.

We can also have quadrilateral strips:

![quadrilaterals strip](https://i.ibb.co/jbrQyMQ/image.png)

Using polygon, we can model the scene with some hierarchy semantics:

```mermaid
graph LR
Object --> Surfaces --> Polygons --> Edges --> Vertices
```

Current reconstruction of a complex scene manually from photographs and other measurements can take 20 persons months to complete and people are looking for ways to make it faster. Some current techniques include laser scanning, interactive modelling. An important area of ongoing research (see COMP37111) is capture geometry via images/video.

![capture geo via image](https://i.ibb.co/25KxgYP/image.png)

##### Drawing a line

For each line on the scene, we sample the true geometry and paint the nearest pixels.

![bresenham algorithm](https://i.ibb.co/tZkPTN1/image.png)

There are many ways to scan-convert a polygon from 3D coordinates $xyz$ to 2D screen in the viewing pipeline, here we introduce the sweep-line algorithm. It keep track of two points that starts from the same point $(x_1,y_1)$, repeatedly scan in a direction, calculate next location of the two point using the gradient and fill the line between them.

![sweep-line algorithm](https://i.ibb.co/fqJ3kc3/image.png)

This is efficient because we only need to calculate the slope of the edges once at the beginning $px, qx$. But it is a floating point algorithm that requires rounding in every iteration.

##### Hidden Surface Removal

We need to determine which pixels should be rendered when there is overlapping. We figured out two ways to work with this, calculate the geometry in 3D space and draw the result or figure out which fragment is nearer during scan conversion. The former was attempted but turns out to be extremely hard, so the latter became the standard approach nowadays.

But how do we figure out which fragment is nearer? The answer is use a **Z-Buffer** (aka depth-buffer) to keep track of the z-value of each pixels. When two pixels have very close Z-values, it may result in **Z-fighting** where pixels maybe rendered incorrectly due to limited precision, this can be very bad if there is animation.

![z-fighting demo](https://i.ibb.co/1rP8nvv/image.png)

The solution is simple, we just **offset the pixels by a tiny amount** to ensure one is always over another when calculated. OpenGL has a function called `glPolygonOffset()` to do this easily.

> #### Lecture 4

#### The Duality of Modelling and Viewing

If we want to move an camera upward by 10 pixels, we can either move the camera (viewing) upward by 10 pixels or move the whole world (model) down by 10 pixels. In computer graphics, **there is no real camera**, so we always move the world (model). So whenever we want to transform the camera by the viewing matrix, we always compute the inverse of the matrix and apply it on the model instead.

#### Defining the Camera Coordinate System

We can find the unit vector axis of the camera using the camera position $E$, center of interest $C$ and the up vector $V$. First we calculate the first axis $\hat{F}=\text{normalize}(E-C)$. Then using the user-specified up vector $V$, we can find  the second axis $\hat{S}=\text{cross_product}(\hat{F},V)$. Then we calculate the last axis $\hat{U}=\text{cross_product}(\hat{F}, \hat{S})$, $\hat{U}$ is essentially the user-specified $V$ but we cannot rely on user giving a correct $V$ that is perpendicular to $\hat{F}$.

![coordinate1](https://i.ibb.co/R4FMc42/image.png)

![coordinate2](https://i.ibb.co/pxYGpxH/image.png)

![V coordinate](https://i.ibb.co/686B3FL/image.png)

> #### Lecture 5

#### Projection

In Computer Graphics, Parallel Projection is mainly for scientific purpose (CAD, Engineering Drawing) due to its precise measurement; Perspective measurement is commonly used due to its sense of realism.

<img src="https://i.ibb.co/XkHKTYy/720px-Graphical-projection-comparison.png" alt="wikipedia projections comparison" style="zoom: 67%;" />

<img src="https://i.ibb.co/GC7b5qS/640px-Various-projections-of-cube-above-plane-svg.png" alt="wikipedia orthographic projection" style="zoom:67%;" />

##### Orthographic (Parallel)

This is project a 3D object onto a plane parallelly; projectors and one of the axis are perpendicular to the projection plane, it can be done by simply discard one of the dimension: 
$$
\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&0&0\\
0&0&0&1\\
\end{pmatrix}
$$
This is same as scaling by $(1,1,0)$. The resulting matrix is **singular** and have two of the three dimensions

![orthographic projection](https://i.ibb.co/CngtW52/image.png)

![orthographic perspectives](https://i.ibb.co/Qch75JB/image.png)

##### Axonometric (Parallel)

It is a parallel projection just like orthographic, projectors must be parallel to the projection plane but axis does not need to be perpendicular to the projection plane.

![axonometric projections](https://i.ibb.co/GHVw9jP/image-1-LI.jpg)

There are three types of axonometric projection, **isometric** that restricts all three angles must be same, **dimetric** that restricts two angles must be same and **trimetric** that does not have restriction on angles.

##### Oblique (Parallel)

This is the most general parallel projection, object and projection plane can be anywhere, the only thing restricting this is that the projectors must be parallel to the projection plane to keep it a parallel projection.

![oblique projection](https://i.ibb.co/LPcZnXg/image.png)

There are two types of oblique projection, **cavalier** projection (used in art) where $x$ and $z$ angles are perpendicular, $x$-axis and $z$-axis are $1:1$ scaled and the third axis's length is kept, and **cabinet** (used in furniture industry) projection which is similar to cavalier projection except its third axis is half the original length. Note that although both cavalier and cabinet projection's $y$-axis are not 90 degree (it would be axonometric if it is the case), cavalier's angle is usually $30$ or $45$ whereas cabinet's angle is usually $atan(2)$ or $\approx64.3^{\circ}$.

##### Perspective

It is a projection where the number of planes perpendicular to the projection plane determine the number of vanishing points. 2/1/0 perpendicular planes is equal to 1/2/3 point perspective.

![perspective projection](https://i.ibb.co/hY91Bmp/image.png)

Given the a coordinate, we can calculate its position on the projection plane using similar triangle. The following uses example of projecting onto a plane that is perpendicular to the $z$-axis.

![similar triangle calculation](https://i.ibb.co/rZ26mwZ/image.png)

We can represent the resulting matrix as:
$$
\begin{pmatrix}
\frac{x}{z/d}\\
\frac{y}{z/d}\\
d\\
1
\end{pmatrix}
$$
where $d$ is the distance from the origin to the $z$-axis.

##### Field of View

We only render part of the world that our camera can see, by specifying a z-near and z-far we can figure out which part of the world we need to render, it is a **cuboid** for parallel projection and **frustum** for perspective projection:

![perspective projection (frustum)](https://i.ibb.co/XxGhpw4/image.png)

So we need to project all the stuff in this frustum onto a plane, it means we will lose information about the depth, this is not good and we do it another way. We first convert the frustum to a cuboid using **projection normalization (PN)** and then apply orthographic projection on it:

![projection normalization](https://i.ibb.co/3SpN2pf/image.png)

This has the same result except that the $z$-values are preserved before applying orthographic projection, and OpenGL does this automatically for us.

We also need to worry about clipping since we are not rendering the whole world, luckily there are efficient algorithm to do this.

![clipping](https://i.ibb.co/2c5975s/image.png)

> #### Lecture 6

#### Reflection

Now we have some geometries ready to render, there are many things we need to worry about before going from this:

![raw geometry](https://i.ibb.co/hRWxts5/image.png)

to this:

![rendered geometry](https://i.ibb.co/Fg3Wvw9/image.png)

Rendering is about light and there is two illumination models, **local and global illumination**:

![local and global illumination](https://i.ibb.co/NNKMj0N/image.png)

Global illumination is much more expensive to achieve, we need to calculate every object's interaction with all other objects (many researches going on to do it efficiently), but if we do not then we lose some features such as reflection and transparency. Here we only look at local illumination.

There is two types of reflection, **diffuse and specular**. Diffuse reflection absorb some wavelength and uniformly reradiate others, this make up the color of the object. Specular reflection reflect what it received, reflected light is same as the light source (not same really, but as an approximation); there is two types of specular reflection, **perfect and imperfect**, example is mirror and ceramics (陶瓷) respectively.

![perfect diffuse reflection](https://i.ibb.co/5997wGd/image.png)

![perfect diffuse reflection](https://i.ibb.co/DRLvDvm/image.png)

![perfect specular reflection](https://i.ibb.co/n1Qdkn9/image.png)

![imperfect specular reflection](https://i.ibb.co/KjNTHbz/image.png)

#### Illuminations

##### Ambient

Whenever a light source is place in an environment, light will quickly fill the enclosing space, there will be light everywhere! Lets assume the lightness is the same everywhere.
$$
I_{ambient} = k_aI_a
$$
where $I_a$ is the intensity, $k_a$ is the **coefficient of ambient reflection** and $0\le k_a \le 1$, it determine what is the brightest value of ambient reflection for this surface.

![ambient illumination](https://i.ibb.co/8BBM6Nw/image.png)

##### Diffuse

There is at least two problems, firstly, we did not consider the angle and distance of the light coming onto the object. Let's take these into consideration.

![directional lightning](https://i.ibb.co/QQ8Qchg/image.png)

We can easily see that when light is $90^{\circ}$ to the surface's normal, it is the most intense, and it degrades as the angle approaches $0^{\circ}$. The function with this property is $\cos$ and here we have the **Lambert's Law**:
$$
I_{ambient} = I_a\cos(\theta)
$$
where $\theta$ is the angle of the incoming ray and the normal. The cosine of the angle between two vectors is the dot product between them, so it is easily calculated. Now our model looks like:
$$
I_{ambient} = I_a k_d (\hat{N}\hat{L})
$$
where $0\le k_d \le 1$ is the **diffuse reflection coefficient** of the surface, it determine what is the brightest value a surface can get for diffuse reflection.

![diffuse coeffcient](https://i.ibb.co/HY8BLxf/image.png)

Next we take care of the distance between the object and the light source. Obviously the longer distance the dimmer the light, so we have to divide by some value, the equation is:
$$
I_e = \frac{I_p}{4\pi d^2}
$$
The reason for $4\pi d^2$ is that the light from a source usually come out as a 3D sphere shape and the distance is squared for back and fore object. But it turns out that although this equation is correct, it does not work well with computer graphics because computer does not have enough precision, so we use an modified version of it in OpenGL instead:
$$
I_e = \frac{I_p}{k_c+k_ld+k_qd^2}
$$
We just tune $k_{\text{constant}},k_{\text{linear}},k_{\text{quadratic}}$to get a nice result. Combined, our model now looks like:
$$
\begin{align*}
I=&\text{ambient} + \text{diffuse}\\
=&k_aI_a+\frac{I_p}{k_c+k_ld+k_qd^2}k_d(\hat{N}\cdot\hat{L})
\end{align*}
$$
where $I_a$ is refers to ambient light intensity and $I_p$ refers to original light intensity. Note that our ambient term is not modified by distance because by definition it is everywhere.

Our rendered image looks like:

![diffuse reflection rendering](https://i.ibb.co/sVYFshb/image.png)

##### Specular

Now with ambient and diffuse lightning taken into consideration, we get the 3D feel of the objects, next we are going to tackle another problem --- texture. The specular reflection we are going to add to our model is affected by the texture of the object, and it is modelled as follows.

![modelling specular reflection](https://i.ibb.co/dMd85zm/image.png)

Past experiments shows that specular reflection varies with three parameters: $\theta, \phi, \lambda$ where $\lambda$ is the wavelength. What we want is a function $S$ that calculate the specular intensity: $I_{\text{specular}}=S(\theta,\phi,\lambda)$. One of the variations is the **Fresnel's Equation**:
$$
F=\frac{1}{2}\left[\frac{\sin^2(\phi-\theta)}{\sin^2(\phi+\theta)}+\frac{\tan^2(\phi-\theta)}{\tan^2(\phi+\theta)}\right]
$$
where $F$ is the fraction of light reflected and $\sin(\theta)=\sin(\frac{\phi}{\mu})$ where $\mu$ is the wave length $\lambda$ dependent refractive index. However, this is kind of expensive, in this example, we will stimulate the **plastic** texture and we can replace all that with a constant $k_s$ (specular reflection coefficient) and use **Phong's specular function** $\cos^n(\phi)$ where $n$ is a hyperparameter to approximate the value.

![phong's specular function graph](https://i.ibb.co/C89jgcg/image.png)

![phong's specular equation](https://i.ibb.co/D5Jkz6m/image.png)
$$
I_{\text{specular}}=I_p(\hat{R}\cdot\hat{V})^n
$$
We can notice that as angle $\phi$ getting larger, the observer will receive less specular reflection. This fits to our understand of light.

Together, our model looks like:
$$
\begin{align*}
I_{\text{specular}} =& \text{ambient}+\text{distance(diffuse+specular)}\\
=&k_aI_a+\frac{I_p}{k_c+k_ld+kd^2}\left[k_d(\hat{N}\cdot\hat{L})+k_s(\hat{V}\cdot\hat{R})^n\right]
\end{align*}
$$
![diffuse and specular rendering](https://i.ibb.co/3RPd1sF/image.png)

![varing kd and ks](https://i.ibb.co/nQfRxyB/image.png)

![varying ks and n](https://i.ibb.co/f1QG5jy/image.png)



# Miscellaneous

## [Vector Graphics](https://en.wikipedia.org/wiki/Vector_graphics)

Vector graphics are computer graphics images defined in term of 2D points. These points are connected by lines and curves to form many other shapes. Each point has a defined position and determine direction of path, path has various properties such as thickness and color.

#### [Vector Monitor](https://en.wikipedia.org/wiki/Vector_monitor)

In the early days vector monitor is used to display graphics. it is a type of CRT ([Cathode-Ray Tube](https://en.wikipedia.org/wiki/Cathode-ray_tube)). It consists of some electron guns which shoot electrons to a monitor made of [special material](https://en.wikipedia.org/wiki/Phosphorescence) which does not immediately re-emit radiation it absorbs and glows (it is used make stuffs glow in dark as well).

### Matrix

$$
\begin{pmatrix}
\text{x coeff} & \text{y coeff} & \text{z coeff} & \text{x bias}\\
\text{x coeff} & \text{y coeff} & \text{z coeff} & \text{y bias}\\
\text{x coeff} & \text{y coeff} & \text{z coeff} & \text{z bias}\\
\text{0} & \text{0} & \text{0} & \text{1}
\end{pmatrix}
*
\begin{pmatrix}
\text{x}\\
\text{y}\\
\text{z}\\
\text{1}
\end{pmatrix}
$$



## History

### Display

During 1950s, people use **Vector CRT display** to create image, it uses electron beam to draw lines and texts and nothing else. Those images fades out almost immediately and required to redraw repeatedly. Nowadays we use **Raster graphics displays** --- 2D array of pixels to display images. Pixels are **approximation** of the real images, the more samples we have the higher fidelity (保真度). 

**Note** that everything in graphics are just approximation, but some are better than others.

### Mesh Data Structure

$f \overset{index\ into}{\rightarrow} e \overset{index\ into}{\rightarrow} v$

- vertex list
- edge list
- face list

### 3D Model Pipeline

1. 3D Vertex
2. Model Transformation
3. Viewing Transformation
4. Projection Transformation
5. Clip to view volume
6. Perspective Division
7. Viewport Transformation
8. 2D Pixels

### Graphics Pipeline

1. Model & View Transformation
2. Lighting (Gouraud / Phong)
3. primitive assembly (clipping)
4. Rasterization (aka scan conversion)
5. Fragment Operation (blending / depth test / surface removal)

Note that in Programmable pipeline, 1 and 2 are replaced by vertex shader program and 4 is replaced by fragment shader program. They must be provided by the user and written in shading language such as GLSL.

### Fixed vs Programmable Pipeline

- Fixed pipeline is deprecated, new stuff cannot be added, but it is easy to learn and fine for many purposes
- Programmable pipeline offers maximum flexibility, state-of-the-art but it has significant start up cost.  

## Light

- Diffuse: pigment particles affect reflection
  - Lambert's law: $I_{\text{effective intensity}}=I_{\text{source intensity}}cos \theta$.
- Specular: pigment particles are not involved, no change of color

![incident angle and wave length image](https://i.ibb.co/zx27x87/screenshot-online-manchester-ac-uk-2020-01-29-20-37-51.png)

![gouraud interpolation image](https://i.ibb.co/gJ7t1k7/screenshot-online-manchester-ac-uk-2020-01-29-20-39-29.png)
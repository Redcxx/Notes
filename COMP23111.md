DBMS stands for database management system

data is information

DBMS is a software which manage/manipulate information

database system is DBMS with data

Logical Data modelling refers to a process obtaining a graphical representation of database without physical implementation, such as UML (unified model language) and ER (entity relation)

A logical model is a model provide various information about entity and relationship between entities in the database

Database schema is a set of specifications about attributes, relations and entities of the database but not the data.

database operations includes (CRUD) create, rename, update and delete

There are many Database Language: Data Manipulation Language (insert update delete), Data Query Language (select), Data Definition Language(create, alter, drop, rename).

Currently sql is the domain language (Structure Query Language) and noSQL is raising (Not only SQL)

DBMS is preferred over file system because it provide a standardize way to manipulate data.

A conceptual model such as ER and UML is suitable for mapping to a logical model such as relation model. This means to create a database schema/model from the diagram.

ER is make up of attributes, entities and relations.

- simple/composite
- single/multi valued
- derived/stored
- N/A, not present, not known

A **cardinality ratio constraint** specific how many relation instance can the entity participate in, 1:1, 1:m, 1:n and n:m

A **participation constraint** specify whether every instance of a entity need to participate in a relation. It can be total or partial

A **weak entity** type do not have a key on their own and relies on other entity for existence. The relation which identify its existence is called a identifying relation and the weak entity type always have *total participation*. The entity it relies on is called the **owner** of the weak entity.

| Basis For Comparison | Strong Entity                                               | Weak Entity                                                  |
| -------------------- | ----------------------------------------------------------- | ------------------------------------------------------------ |
| key                  | primary                                                     | partial discriminator                                        |
| Depends              | Independent of any other entity                             | Depends on the strong entity                                 |
| Denoted              | single rectangle                                            | double rectangle                                             |
| Relation             | two strong entities is denoted by a single diamond          | a weak and a strong entity is denoted with a double diamond  |
| Participation        | May have total or partial participation in the relationship | Always has total participation in the identifying relationship (double line) |

​     

- key: underline
- weak key: dash underline
- multi-value: two circle
- composite: connect by other lines
- derived: dash circle
- attribute: circle
- entity: square
- partial participation: single line
- total participation: double line
- relation: diamond
- identifying relation: double diamond

A recursive relation is a relation which relates to itself.



EER: a high level model which extends the original ER and provides more functionalities

- attribute/relation inheritance
- category/union type (a entity can belong to one of more sub classes)
- super/sub class (disjoint/overlap)
- specialization/generalization (useful capturing semantics, more accurate and often more efficient)

Note that a entity cannot exist merely being a member of any super class it has be one of its subclass which inheritance attributes from the superclass.

Advantage of using relation model

- easy to use/understand (no need to navigate into structure and consist of row and columns only)
- scalable (we can just add data by append to a table)
- data independent (we can change structure without changing application)
- structure independent
- query capability

Disadvantage of use relation model

- Field lengths are limited
- database can be complex to see relationship between tables
- hard to exchange information between database

Relational Language

Unary

- select $\sigma <logic> <(table)>$
- rename $\rho (<new>, \pi(old)(table))$
- project $\pi <field, ...> <(table)>$

Binary​

- JOIN 
  - $\text{theta join} \bowtie_{\theta}$
  - equi join $=$
  - natural join $\bowtie$
  - left/right outer join $⟖$
- DIVISION

Set Theory

- union $\cup$
- disjunction $\cap$
- set difference $-$
- cartesian product $\times$

theta join can use any criterion in the selection criteria, when it only use equivalence condition it becomes equi join.

Relation Mapping Algorithm

Note that disjoint/overlap have the same primary from their parent but union's children have their own primary key and use foreign key link to their parent.

1. Strong ((composite) key, simple attributes)

2. Weak (primary/foreign/partial key)

3. 1:1

   - Foreign Key: include a foreign key in one of the relation to another's primary key, use total participation if can.
   - Cross-Reference/Relationship Relation: use a new table to connect two relations.
   - Migrated Relation: merge two relation to form a new one

4. 1:N

   - Foreign Key: add foreign key in the N relation
   - Can use cross-reference but rarely

5. N:M

   - Cross-Reference/Relationship Relation: use a new table to connect two relations.

6. multi-value (add its simple attribute if it is composite)

7. N-ary

   - create new table for them

8. disjoint/overlap

   - Multiple relation superclass and subclasses:
     - For any specialisation (total or partial, disjoint or overlapping)
     - child use primary key link to parent and child have its own attribute
   - Multiple relation subclasses only
     - for total and disjoint (overlap can cause duplication)
     - child include all parent attributes
   - Single relation with one type attribute
     - for disjoint and potential for many null if attribute is only one of the subclass
     - parent include all children attributes, only the specific type's attribute field has value others NULL (this can generate many NULL values)
   - Single relation with multiple type attribute
     - for overlap but also work for disjoint
     - have a extra flag to specify whether this instance is this type

9. union

   - create relations for that category which includes a foreign key to that category.

   

Functional Dependency

- It helps to identify good and bad database design
- remove data redundancy
- avoid insert/update/delete anomalies
- maintain quality/ define meaning/ find facts/keys

If the database is designed properly all its attributes will only depends on the key.

constraint between two sets of attributes in a relation $X \rightarrow Y$: $Y$ depends on $X$

Armstrong's Axiom

- Transitivity: $A \rightarrow B \text{ and } B \rightarrow C \text{ implies } A \rightarrow C$
- Reflectivity: $AB \subseteq ABC \text{ implies } ABC \rightarrow AB$
- Augmentation: $A \rightarrow B \text{ implies } AC \rightarrow BC$

Inference Rules from Armstrong's Axiom

- Pseudo Transitivity: $A \rightarrow B \text{ and } BC \rightarrow D \text{ implies } AC \rightarrow D$
- Union: $A \rightarrow B \text{ and } A \rightarrow C \text{ implies } A \rightarrow BC$
- Decomposition: $A \rightarrow BC \text{ implies } A\rightarrow B\text{ and } A \rightarrow C$

$X^+$

$X^+$ (X closure) refers to the set of all attributes that $X$ can infer given a set of functional dependency

$F^+$

$F^+$ (F closure) refers to the set of all functional dependencies can be logically implied given a set of functional dependencies.



Database

multi user and single user --- whether can access the same time, single core machine can do this by interleaving

database item: various granularity (粒度) can be table, record and field.

We need concurrency control because:

- update can be lost
- temporary update (dirty read)
- read maybe unrepeatable
- summary maybe incorrect

Transaction can be committed or aborted

It can be cancelled due to

- system failure --- hardware/software/network/disk error
- transaction error --- divide by zero, integer overflow
- logical error --- data not found, balance goes negative after withdraw (constraint)
- concurrency control enforcement --- deadlock, violate serializability (concurrent outcome should be same as serial outcome)
- physical problems --- fire, theft, sabotage

transaction need to know when it begin, what to do if fails, when it ends, if it is successful ...

Transactions should have ACID properties

- Atomicity: completes or does not happen
- Isolation: it happens as if it is working in isolation even if it is concurrent
- Consistency Preservation: database should go from a consistent state to another
- Durability or Permanency: changes made should not be lost due to failures after it

A schedule is define as $S$ of $n$ transaction $T_1, T_2 \dots T_n$ where $n$ also denote the order of operations and these operations will interleaved into $S$

Begin, Read, Write, Execute, Commit, Abort

<img src="https://i.ibb.co/HzBhw4H/image.png" alt="schedule-image" style="zoom:80%;" />

Conflicting operation

- different transactions
- use the same data
- at least one of them is write

It can result in

- Incorrect summary --- summary over repeated data and get wrong result
- lost update --- update and overwritten other transaction's update
- temporal update --- aborted and rerolled changes of other transaction
- non-repeatable read --- read the same data repeatedly and get different result

This result in inconsistent data, we want result to be equivalent (sometimes two different schedules can accidentally produce the same result)

- Two schedules are equivalent if the operations applied to each data item affected by the schedules should be applied to that item in both schedules in the same order

- The most common approach to equivalence of schedules is the conflict equivalence 

- Two schedules are said to be conflict equivalent if the relative order of any two conflicting operations is the same in both schedules.
- S is serializable if its (conflict) equivalent to some serial schedule

Use graph theory to check to test if a schedule is serializable

- if $A$ read before $B$ write, creates $A \rightarrow B$ 
- if $A$ write before $B$ write, creates $A \rightarrow B$
- if $A$ write before $B$ read, creates $A \rightarrow B$

If the graph does not forms a cycle then the schedule is serializable.

Main objective in developing a logical data model for relational database systems is to create an accurate representation of the data, its relationships, and constraints.

In normalization attributes on the left has 1:1 relationship with attributes on the right. This prevent update anomalies.

![image-20200123131334830](C:\Users\wweilue\AppData\Roaming\Typora\typora-user-images\image-20200123131334830.png)

Normalization a serial of steps of a formal technique for analysing relation based on the primary key and functional dependencies between attributes. Each step corresponding to a specific normal form.

The higher the normal form, the more tables we need

- data split across tables
- performance issue as we need to go through tables instead of one
- complex views and joins

A relation that is in 1NF and every non-key attribute is fully functionally dependent on the primary key.

BCNF

- non-key attribute cannot derive key attribute

UNF

- Unnormalized table

1NF

- no repeating attributes (create new table if any)
- column values are atomic
- has a primary key
- (copy primary key, compound key) ? 

2NF

- 1NF and every non-key attributes is fully functionally dependent on the primary key. (no partial dependency)

3NF

- 2NF and no non-key attribute is transitively depend on the primary key. (new table if any)

Store Procedure

It is a stored declarative SQL statements in the SQL server. It can be invoke by the CALL keyword. When it is called for the first time, the name will be look up in the database catalog and compile the code and save in the cache. Next time you call it (in the same session) will just execute the cached code.

advantage

- security - grant privilege to procedure instead of table
- less bandwidth - client send name of procedure instead of statements
- consistency - provide standard way to achieve a purpose

disadvantage

- more memory needed for server --- need to store procedures
- hard to debug --- SQL does not provide debug tools
- hard to maintain --- required specialized skillset

create procedure [name]

drop procedure [name]

select [name] into [var]

IN --- read

OUT --- write

INOUT --- read & write

call getnum('name', @num<this is otuput variable>)

declare [name] INT DEFAULT [value]

You cannot change procedure directly except using some application such as phpMyadmin and MySQL Workbench

if then elseif else

case when then else

loop leave --- [name]: loop, leave [name], end loop [name]

 while do

repeat until

declare [cursor_name] cursor for [sql statement]

open, close



show triggers, drop trigger [name], create trigger [name] before/after update/insert/delete on [table_name] <for each row> (?)

delete cannot access NEW

````mysql
CREATE TRIGGER before_products_update_log_user
  BEFORE UPDATE ON products 
  FOR EACH ROW 
  **FOLLOWS** **before_products_update**
BEGIN
-- other code to handle this trigger
````

![image-20200123164833310](C:\Users\wweilue\AppData\Roaming\Typora\typora-user-images\image-20200123164833310.png)

![image-20200123164839553](C:\Users\wweilue\AppData\Roaming\Typora\typora-user-images\image-20200123164839553.png)

hard disk 

ring = track

slice = sector

part of ring in slice = block

part of ring in slices = cluster

access block using sector and track number

index --- metadata for navigate, such as primary key (primary index), when we need another index for something we frequently look up for is call a secondary index. index is nice and they are store in disk as well, hopefully in one single block, if not then we need a two-way partition which form a tree like structure for faster navigation.

primary index --- <K, P>

the first block in each block of the data file (P is pointing to this) in called the anchor record.

Primary is sparse index!!!

A dense index has an entry for every record in the data file (block). A sparse index has an entry for a number of record in the data file.

dense index is faster but sparse index required less storage and impose less maintenance

clustering index is a primary index which is a sparse index that has a index for all attributes that share the same value.

secondary index is a non-key and non-ordered index. Thus we can only create dense index for it, so it has longer search times and require more space. Note that although the table is not ordered, the index will be, so we can do binary search on index.

secondary index implementation

- Option 1 is to include duplicate index entries with the same K(i) value—one for each record. This would be a dense index. 

- Option 2 is to have variable-length records for the index entries, with a repeating field for the pointer. We keep a list of pointers in the index entry for K(i)—one pointer to each block that contains a record whose indexing field value equals K(i). 
- Option 3, which is more commonly used, is to keep the index entries themselves at a fixed length and have a single entry for each index field value (has extra level, binary search can work without customization)

Indexing schema we learnt are all order and we can apply binary search

because the second level is a primary index, we can use block anchors so that the second level has one entry for each block of the first level

- B-Trees, contain pointers to data at various levels

- The values are not repeated

- B^+-Trees we have to retain the middle values to guide the search, this value points to the next block

- Using B^+-Trees we end up with all data pointers at the leaf nodes. Which allows simple creation of a sequential linked list

2NF: remove partial dependency

3NF: remove transitive dependency
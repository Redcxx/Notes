COMP36111 Algorithms & Computation

## Lecture 1 Strongly Connected Component

#### Definition

A graph is G = (V, E). V = **finite** and non-empty set, E = set of order pair of **distinct** v. V are **vertices** and E are **edges**.

If e = (u,v) in E then u and v are **neighbor** and incident on e.

- no self-loop
- no multiple edges
- no directionless edges

By default we assume graph is stored in adjacency list, where each vertices associated with a linked-list (because size is small and variable) of other vertices where it has an outgoing edge.

- in-degree: # incoming edges
- out-degree: # outgoing edges
- path: sequence of distinct vertices such that if i < j, then (vi, vj) is an edge
- reachable if there exists a path
- strongly connected if every v is reachable from every other.
- topological sorting: an order of v such that for all (vi, vj) we have i < j.
- strongly connected  component: every node is reachable from other in that component.
  ![image-20201010141538697](C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201010141538697.png)

- st-con: test if t is reachable from s in g
- strong connectivity: test if g is strongly connected
- cyclicity: test if g contains cycle

- topological sorting: compute topological sorting of g
  - by product: check contains cycle
- strongly connected component: find strongly connected components of g

- lemma: directed graph + no cycles = v with 0 in-degree



- dfs O(e+v)

  ```
  def dfs(g, v):
  	mark v
  	for each (v, u) in g:
  		if u is not marked:
  			dfs(g, u)
  ```

- topological sorting O(e+v)

  ```
  def t_sorting(g):
  	list;
  	for v in g:
  		if in-degree of v == 0:
  			list.add(v)
  			g.remove(v)
  	if list is empty:
  		print "contains cycle"
  	else:
          list.addAll(t_sorting(g))
      return list
  
  def t_sorting(g):
  	list, map, stack;
  	for v in g:
  		map[v] = in-degree of v
  		if map[v] == 0:
  			stack.push(v)
  	while stack is not empty:
  		# when v is popped, all its predecessor must be popped already
  		# so it is added after its predecessor is added
  		v = stack.pop()  
  		list.add(v)
  		for (v, u) in g:
  			if --map[u] == 0:
  				stack.push(u)
  	if list.size() != g.size():
  		# if some v is not indexed, then it has cycle
  		return "contains cycle"
  	else:
  		# if all v is indexed, then we have a topological sorting
  		return list
  ```

- strongly connected component (tarjan's algorithm)

  ```
  def scc_tarjan(g):
  	list of scc;
  	for v in g:
  		if v.index undefined:
  			sc(g, v, 0, new stack, list of scc)
  	return list of scc
  
  def sc(g, v, index, stack, list):
	v.index = index
  	v.lowlink = index
  	v.on_stack = true
  	stack.push(v)
  	
  	for (v, u) in g:
  		if u.index undefined:
  			sc(g, v, index+1)
  			v.lowlink = min(v.lowlink, u.lowlink)
  		else if u.on_stack:
  			v.lowlink = min(v.lowlink, u.index)
  			
  	if v.index == v.lowlink:
  		scc = new list;
  		repeat:
  			w = stack.pop()
  			w.on_stack = false
  			scc.add(w)
  		while w != v
  		list.add(scc)
  	return list
  ```
  
  

## Lecture 2 Union Find

#### Pre Lecture 2 Dijkstra Summary

- Task: find shortest path from a node to all other node in a graph

- Priority Queue: Node closer to the source is searched first

- Path Representation: each node in p-queue represent the current shortest path (distance to source and parent)

- Relaxation: when a shorter path is found, its distance and parent is updated.

  

- If weight is the same, then it is same as bfs.

- To get from source to only one destination node, stop when the destination node is popped.

- Completeness: If there is a shortest path, it will find it.
- Optimal: It always find the most optimal (shortest) path.
- Uninformed: It does not use any information about the location of the target.

#### Content

Union find is an algorithm operate on undirected graphs. A partition of V is pairwise disjoint sets that when taken together, it forms V. We refer to elements of a partition as cells. 

- makeSet(v): create singleton set containing v.
- union(Ai,Aj): remove Ai and Aj from P and add new cell Ai U Aj.
- find(v): return pointer to cell of P containing v.

```python
def union_find(V,E):
	P = empty partition
	for v in V:
		P.add(makeSet(v))
	for (u,v) in E:
		if P.find(u) != P.find(v):
			P.union(P.find(u), P.find(v))

def makeSet(v):
	s = [v] # optimization2: use tree instead
	s -> size = 1
	v -> cell = s
	P.add(s)

def find(v):
	return v -> cell
	
def union(s,t):
	if s->size < t->size:  # optimization1: use the smaller size list
		s,t = t,s
	P.remove(t)
	s = append(s,t)
	for v in t:
		v->cell = s
	s->size += t->size
```

lemma: no element can assigned more than low_bound(logn) +1 times
proof: whenever v->cell changes, the size of cells at least doubles, but 1<=size<=n, this can only happen lower_bound(logn) times.

theorem: union-find is O(e+vlogv)
prrof: makeSet, union and find is linear: e+v, they are constant time except updating v->cell, this update is at most logn so yielding total work O(e+vlogv+v).

optimization2: use tree to store cells, and parent is the root of the cell, when merge, just point two partition to the same parent.
<img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201019111943861.png" alt="image-20201019111943861" style="zoom:67%;" />

```python
def makeSet(v):
	v -> parent = v
	v -> size = 1
	return v

def union(u,v,P):
    ...
	v -> parent = u
	...

# we have to change find so that it return the parent
# this is logn but we can flatten it
def find(v):
	if v->parent != root:
		v->parent = find(v->parent)
	return v->parent
```

With tree implementation, the union-find algorithm runs in (almost linear) O((e+v)alpha(v)) where alpha is a very slow growing function.

## Fast & Slow

### Fast

- polynomial (in x): expression p(x) of the form: $a_nx^n+a_{n-1}x^{n-1}+\dots+a_1x+a_0$. 

- polynomial bounded: a function is polynomial bounded if for some p: $\forall n:f(n)\leq p(n)$.

- exponentially bounded:

  <img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201019113501284.png" alt="image-20201019113501284" style="zoom:50%;" />

  <img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201019113533303.png" alt="image-20201019113533303" style="zoom:50%;" />

  - $2^n$: singly exponentially bounded

  - $2^{n^2}$: singly exponentially bounded

  - $2^{2^n}$: doubly exponentially bounded

  - $n!$: between singly exponential and doubly exponential:
    by taking log on both complexity:
    $$
    \log(2^{2^n}) = 2^n\\
    \log(n!) \leq \log(n^n)=n\log n\\
    n\log n \leq 2^n
    $$

  - <img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201019121614394.png" alt="image-20201019121614394" style="zoom:50%;" />

  LOOP language:

  ```python
  x=y
  x=0
  x++
  return x
  loop (x) {...} # x is fixed even if changed in loop
  ```

  <img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201019125352765.png" alt="image-20201019125352765" style="zoom: 50%;" />

![image-20201019125638260](C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201019125638260.png)

- any function in L2 (exponential) is elementary, note that L3 is tetration functions, so elementary means it has fixed amount of towers.

- any function in the set: $\bigcup_{i=1}^{\infty}\mathcal{L}_i$, is primitive recursive. e.g. tetration function is in L3, so it is not elementary, but it is primitive recursive

- There exists functions that are not primitive recursive (i.e. cannot compute by LOOP language) but it is a total computable function: Ackermann function:
  $$
\begin{align*}
  A(m,n)=\begin{cases}
n+1 & m=0\\
  A(m-1,1) & m\gt0 \text{ and } n=0\\
  A(m-1, A(m,n-1)) & \text{otherwise}
  \end{cases}
  \end{align*}
  $$
  This function grow rapidly, much faster than any functions in LOOP language.
  
- notation: $A^{(k)}(x)=\overbrace{A(A(\dots A(}^{\text{k times}}x)\dots))$
  
- An Ackermann-like function:
  $$
\begin{align*}
  A_0(x) &=x+1\\
A_{i+1}(x)&=A_i^{(x)}(x)\\
  A(x)&=A_x(2)
  \end{align*}
  $$

### Slow

$$
\alpha(n)=\min\{i\ge 0:A(i)\ge n\}\\
\text{Note: }\alpha(A(n)) = n
$$

alpha(n) is the minimum i such that A(i) is more than or equals to n. Minimum increasing i such that A(i) >= given number. So if A(i) is fast growing, then alpha must be slow growing, because for the number i to increase, A(i) must be smaller than given n.

When given the number of atoms in the universe, it yields 4.

## Lecture 4

### Flow

flow network = (V, E,s,t,c), VE=directed graph, s=start vertices (no incoming edges), t=end vertices (no outgoing edges), c=function that assign edge to natural number (capacity). edge's capacity has no negative capacity. 

A flow in N is f:E->R+ with two properties:

-  conservation rule: net flow out of any e is zero: $\sum_{v:(u,v\in E)}f(u,v)-\sum_{v:(v,u)\in E}f(v,u) = 0$. (each vertex does not store any water)
- capacity rule: no edge exceed capacity. (pipe cannot contains more water than its capacity)

### Cut

Cut is a separation of vertices into two parts $(V_s, V_t)$. For every bridge between $V_s$ and $V_t$  (edges that connected these two partitions):

- It is forward edge if it goes from $V_s$ to $V_t$.
- It is backward edge if it goes from $V_t$ to $V_s$.

**flow across a cut** is the net flow of it, i.e. forward flow - backward flow.

- flow across cut = flow of network
- flow of cut <= capacity of cut
- any flow of network <= capacity of any cut

If we can find the minimum cut (cut that has the least capacity), then we can have a upper bound for the maximum flow of the network. In fact, the maximum flow of a network is equal to the capacity of the minimum cut.

### Residual Capacity

It is just capacity - flow: $\Delta_f(u,v)=c(e)-f(e)$.

### Ford-Fulkerson Algorithm

auxiliary directed graph: $\{(u,v)\in E | f(u,v) <c(u,v)\} \cup\{(v,u)|(u,v)\in E \and f(v,u)>0\}$.

- remove exhausted edges (max flow)
- add backward edge for every non-zero flow edge

if you can find a path in auxiliary directed graph, then you can increment the forward edge and decrement the original edge of the backward edge path to add 1 more to the over all flow. In the book, such path is called augmenting path.

if you cannot find a path in auxiliary directed graph, then it already has the maximal flow.

```python
def ford_fulkerson(N):
	assign 0 flow to each edge
    while augmenting/auxiliary path exists as path:
        min_rc = min residual capacity of all edges in path
        for each edge in path:
            if edge is forward:
                edge.flow += min_rc
            else:
                edge.flow -= min_rc
```

### Cost

Cost of an edge is the cost for one unit of flow.

Finding min-cost max-flow is same as finding the shortest path in all max-flows.

If we have a negative weight cycle then we can always find a path that has less weight but with the same flow, so we have a negative cost cycle if and only if we do not have a minimum cost flow. Note that we can modify the edges so that all of them has positive weights, then we can use dijkstra's algorithm instead of bellman ford's algorithm.

We just need to find the shortest path instead of any augmenting/auxiliary path in the while loop above.

## Miscellaneous

Total function: function defined for all possible input values

Computable function: function that can write a algorithm to compute it.

#### Quiz

- SAT is PTime-hard: **True**
- PSpace is a proper subset of ExpTime: **Not Known**
- PTime = co-PTime: **True**
- PTime is a subset of NPTime ∩ co-NPTime: **True**, opposite direction is not known
- PTime is a proper subset of ExpTime: **True**
- PSpace is a subset of NLogSpace: **False**
  - NPSpace = PSpace (in later lecture)
- The problem QBF (truth of quantified Boolean sentences) is in PTime: **Not Known**
  - if this is true, PTime = PSpace

#### Amortied Analysis

amortied analysis is a method for analyzing time/space complexity, it tries to looking at the worst-case run time per operation, rather than per algorithm, as it can be too pessimistic.

- Avergae running time per operation over a worst-case sequence of operations.
- traditional worst case analysis can give too pessimitic bound if the only way of having expensive operation is to have a lot of cheap ones before it.

- aggregate method: amortized cost = total cost/#operations
  - inserting in array, we have to expand array if necessary, this means worst case insert is O(n), but amortized analysis says that it is O(1), since when cost of expanding the array is spreaded among elements, it is constant.
- Accounting method: Some operations are overcharged, because the cost maybe used for paying subsequent operations.
  - Amortied analysis says that inserting in a array is 3 credits:
    - table not full: use 1 credit for insert and save 2 credits
    - table is full:  doubling: half of the elements have 2 credits each. Use these to pay for reinsertion of all in the new array.
- Potential method: (lazy read too long)

#### Proof union-find complexity is mlogn

- **lemma 1: As the find function follows the path along to the root, the rank of node it encounters is increasing.**
  initially all node is in its own set, it is trivially true.
  find: all nodes visited along the path will be attached to the root, which have higher rank than its children.
  union: a tree with smaller rank will be attached to higher rank.

- **lemma 2: A node *u* which is root of a subtree with rank *r* has at least $2^r$ nodes.**

  intially all node is in its own set, it is trivially true.

  union: every union the size at least doubles, so $2^r+2^r=2^{r+1}$

- **lemma 3: The maximum number of nodes of rank *r* is at most $\frac{n}{2^r}$.**
  from lemma 2 we know rank r has at least $2^r$ nodes, so at most we can have $\frac{n}{2^r}$ such node.

- We define "buckets" here, sets that cotains vertices with particular rank.
  ![image-20201021112631696](C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201021112631696.png)

  we can see that bucket 5 should be enough for any purposes.

- **observation 1: The number of buckets is at most logn**
  Whenever we add one more bucket, we add one more two to the power.

- **observation 2: The number of elements in any bucket is at most n**
  We only have n vertices in total.

- Let $F$ be the find operation. The total cost of find operation is $T_1 + T_2 + T_3$
  $$
  \begin{align*}
  T_1&=\sum_F(\text{link to the root})&=1\\
  T_2&=\sum_F(\text{links traversed between different buckets})&=m\log n\\
  T_3&=\sum_F(\text{links traversed within the same bucket})&=n\log n
  \end{align*}
  $$
  $T_1$ linked to root, so exactly one traversal needed.

  $T_2$ from observation 1 we have that maximum buckets is logn, when path > vertices we have mlogn.

  $T_3$ is dervied from:
  $$
  \begin{align*}
  T_3&\leq\log n(2^B-1-B)\frac{n}{2^B} &\text{not including root because it is in }T_1\\
  &\leq\log n (2^B) \frac{n}{2^B}\\
  &=n\log n
  \end{align*}
  $$
  

Therefore $1+m\log n+n\log n=m\log n$.

#### Proof union find is almost linear

rank(v) = lower_bound(log(number of nodes under(v))) +2

- rank of any v is less than log(n)+2
- parent's rank is always larger
- number of node with rank s is at most n/(2^{s-2})
- every time we merge one tree into another, the r(p(v)) increases because it now has more nodes

L(v) = largest i for: rank of parent of v >= Ai(rank of v), it is the maximal i such that it does not exceed the rank of the parent

- L(v) < a(n)
  - v can be any node, its max is the root of all nodes, so rank(v) is at most n, since L says it is i such that Ai(n) does not exceed n and a(n) is i that just exceed n, so a(n) is always(atleast) 1 more than L(v), so L(v) < a(n)

now we split the computation into two parts:

1. nodes that does not have ancestor with the same label
   since the number of different labels is at most a(n) and find is called for each edges, the time complexity is O(a(n)m).
2. 

#### Simplex

1. convert minimize to maximize problem
2. convert all equations to <= constant
3. convert to table and add slack variables for each constraint
4. repeat until all positive in objective function row
   1. find minimum row and column. min column = least in object function row, min row = min constants/min column, call this number m
   2. convert m to one and all number in m's row to zero (equivalently).
5. then we get the solution by:
   1. for each col:
      1. if it is singular (only contain one 1 and others are 0), then its value is its row's constant
      2. else 0.

### Lecture 5

#### Turing Machine

Turing Machine: $M=\langle K,\Sigma,Q,q_0, T\rangle$, where:

- $K$: >= 2 tapes
- $\Sigma$: non-empty finite set of alphabets
- $Q$: non-empty finite set of states (this refers to the content of the state register, i.e. the head, not including content of tapes)
- $q_0 \in Q$: initial state
- $T$: set of transition

#### Symbols

- Symbol $\in\Sigma\ \cup\{\Huge\textvisiblespace\normalsize, \rhd\}$
- set of finite strings over $\Sigma$ by $\Sigma^*$

#### Transition

A transition = $\langle p, \overset{\bold{-}}{s}, q,\overset{\bold{-}}{t}, \overset{\bold{-}}{d} \rangle$ where:

- $p$: current state
- $\overset{\bold{-}}{s}$: K tuples of symbols (symbols on current K tapes' head, to be scanned)
- $q$: next state
- $\overset{\bold{-}}{t}$: K tuples of symbols (symbols to be written on current K tapes' head, after this the head contains $\overset{\bold{-}}{s}$)
- $\overset{\bold{-}}{d}$: the direction to move after symbols is written $\in \{\text{left},\text{right},\text{stay}\}$.

#### Configuration

A configuration of M (turing machine) is a K tuple of strings $\sigma_i$ where each in the form:

- $\rhd, S_{k,1},\dots,S_{k,i-1},q,S_{ki},\dots,S_{k,n(k)}$
- we put $q$ in front of the symbol where head is currently at.

#### Run

A run of M on input x is a sequence of configurations. For each configuration

- if it is successive, then it conforms to some transition
- if it is not successive(some transition is possible) then it may be a final/initial state.

It is terminating if finite and deterministic if there is only one transition for every state.

- $M\downarrow x$: machine M is terminating on input x
- $M\uparrow x$: machine M is non-terminating on input x

if $M\downarrow x$ then we can have a set of output strings, this is the partial function: $f_M:\Sigma^*\rightarrow\Sigma^*$:
$$
f_M(x)=\begin{cases}
y&\text{if }M\downarrow x\\
\text{undefined}&\text{otherwise}
\end{cases}
$$
Now we can say M computes fM, and a partial function f:sigma->sigma is computable if it is computed by some deterministic turing machine.

We can encode a sequence of numbers using prime-power encoding, thus we can encode one turing machine into a number, so this means we can have another turing machine that takes this encoded turing machine EM as a number and the input x and compute the function EM(x), the machine that takes in EM and x and output EM(x) is called the universal turing machine.
<img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201031094400787.png" alt="image-20201031094400787" style="zoom:50%;" />

It just means a turing machine that can runs another turing machine.

<img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201031094538276.png" alt="image-20201031094538276" style="zoom:50%;" />

if you have a language recognized by some turing machine, then there is a deterministic turing machine (DTM) that recognized the same language (because non-deterministic turing machine can converted into a deterministic one.

- language L from alphabet S is recursively enumerable (r.e.) if there is a DTM which recognizes L.
- language L is co-recursively enumerable (co-r.e.) if there is a DTM which recognizes S \ L.
- language L is recursive if and only if there are DTM that recognize L and another DTM that recognize S \ L.
- language L is recursive if and only if there is a DTM that recognize it and always halts.

- language $\Leftrightarrow$ decision problem.
- recursive $\Leftrightarrow$ decidable.
  - the problem that determine whether a number x is prime is the language:
    $\{x\in\{0,1\}^*|\text{x is prime number\}}$
  - the problem that determine whether formula of propositional logic is satisfiable
    $p_0\and(p_1\rightarrow p_{10})$ is the language $\{x\in\{p,1,0,\and,(,),\rightarrow\}^*|\text{x is a wff and satisfiable}\}$.
    wff: well-formed formula

<img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201031101821588.png" alt="image-20201031101821588" style="zoom:50%;" />

There exists undecidable problem:
<img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201031103429785.png" alt="image-20201031103429785" style="zoom:50%;" />

Turing says that you can code the halting machine H as validity of first order logic, so there is no way to check the validity of first order logic, because if there were, then we have a halting machine which we have shown is not possible

## Lecture 6 Time, Space & Determinism

<img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201102104448009.png" alt="image-20201102104448009" style="zoom:50%;" />

- M runs in time $g: N\rightarrow N$ if any run on a finite set of strings $\Sigma$ halts at most g(|x|) where $x\in \Sigma$.
- M runs in space $g: N \rightarrow N$ if M always terminates and any run on a finite set of strings $\Sigma$ uses at most g(|x|) squares on any of its work-tapes. 

<img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201102110044424.png" alt="image-20201102110044424" style="zoom:50%;" />

- L is in Time(g): exists DTM that recognizing L and run in time g
- L is in Space(g): exists DTM that recognizing L and run in space g

e.g.:

<img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201102110640466.png" alt="image-20201102110640466" style="zoom:50%;" />

- It runs in Time(3n+1) and Space(upper_bound(log n))

<img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201102110810388.png" alt="image-20201102110810388" style="zoom:50%;" />

#### Linear 'speed-up' Theorems

<img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201102111111084.png" alt="image-20201102111111084" style="zoom:50%;" />

Given a turing machine M which recognize the language, you can build a larger turing machine operating on larger set of states/alphabets, in this larger machine, several steps of the old turing machine is compiled into a single step.

because of the theorem, we can simply the complexity:

<img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201102111712558.png" alt="image-20201102111712558" style="zoom:50%;" />

we use big G to denote a large set of complexity, where P is polynomials, and E is exponential

<img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201102111818107.png" alt="image-20201102111818107" style="zoom:50%;" />

Some complexity of recognizing language using turing machine:

<img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201102112030769.png" alt="image-20201102112030769" style="zoom:50%;" />

We usually do not say time(log n) because that means the turing machine does not need to read the whole input which is not very interesting. We also defined some sets of languages(problems) by their complexity here.

<img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201102112346030.png" alt="image-20201102112346030" style="zoom:50%;" />

- st-con: reachability given two vertices, just dfs.
- undirected st-con: same as above, dfs.
- cyclicity: check has cycle
- 2d-matching: check perfect match

We cannot access certain index's element in the array using constant time, where most analysis assumes, but in practice it affect the degree of polynomial but does not change the polynomial time complexity.

Some other polynomial time problem is linear programming feasibility (LPF):

<img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201102113725907.png" alt="image-20201102113725907" style="zoom:50%;" />![image-20201102113817732](C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201102113817732.png)

<img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201102113905700.png" alt="image-20201102113905700" style="zoom:50%;" />

Prime may seen trivial in polynomial time, i.e. just check all digit within sqrt(n), but input we measuring is bit string, so the size of input is not n but log n (the size of digits) and sqrt(n) grow faster than any fixed power including log (n). The reason why it can be tested in polynomial time even with binary coding of input is due to the proof in 2004 but we are not going into it in this course.

<img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201102114425450.png" alt="image-20201102114425450" style="zoom:50%;" />

Similar go for NDTM:

<img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201102114605757.png" alt="image-20201102114605757" style="zoom:50%;" />![image-20201102130050464](C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201102130050464.png)

<img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201102130142802.png" alt="image-20201102130142802" style="zoom:50%;" />![image-20201102130229490](C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201102130229490.png)

<img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201102130241111.png" alt="image-20201102130241111" style="zoom:50%;" />

<img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201102131730220.png" alt="image-20201102131730220" style="zoom:50%;" />

If answer is yes, then there will be a run which return Y, else you can never return Y, so this algorithm can decide feasibility. It is non-deterministic since you can just guessing one tour.

<img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201102132327201.png" alt="image-20201102132327201" style="zoom:50%;" />

<img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201102132633295.png" alt="image-20201102132633295" style="zoom:50%;" />

left is k-colorable but not right.

<img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201102132644194.png" alt="image-20201102132644194" style="zoom:50%;" />

It is ND, because you can just guess a coloring, then check if it is correct, then by some miracle luck you will have it right and return Y. 

#### Inclusion

<img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201102133344967.png" alt="image-20201102133344967" style="zoom:50%;" />

Time(G) is in Space(G) because you can at most reach G squares in the turing machine tape with Time(G).

Let M be a K-tape turing machine $\langle K,\Sigma,Q,q_0,T\rangle$ and size of input is $s(n)$ then the maximum amount of configuration is:
$$
\begin{align*}
\text{max config size}&=\text{possible states}&\times& \text{possible tape content} &\times &\text{possible head}\\
&=|Q|&\times&|\Sigma|^{K\times s(n)}&\times&(s(n)+1)^K\\
&\in2^{O(s(n))}
\end{align*}
$$

- possible states is the value of the head.
- possible tape content is combination of all alphabet of length s(n) on K tapes.
- possible head is possible position for head, +1 because you can be at the start and end of the tap.

This set of configurations forms a directed graph, where each edge represents one configuration can transition to another. We can also find a start configuration $c_0$ and a success configuration $c_*$. We call this a configuration graph G for M with input x. 

<img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201102170505801.png" alt="image-20201102170505801" style="zoom:50%;" />

 <img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201102195122438.png" alt="image-20201102195122438" style="zoom:50%;" />

- 2^{log} is in polynomial time, 2^{poly} is in exp time, similarly for others

<img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201102201337259.png" alt="image-20201102201337259" style="zoom:50%;" />

This basically says use k_i to represent the exhausive list of non-deterministic choice of a run on NDTM. 

<img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201102202015864.png" alt="image-20201102202015864" style="zoom:50%;" />

It basically says that since we have recorded all ND choices using k_i, we can use an extra tape to record each deterministic choice on the non-deterministic choices on each step, so the space use is at most space(g).

<img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201102202251913.png" alt="image-20201102202251913" style="zoom:50%;" />

### Separation Result

<img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201104090405692.png" alt="image-20201104090405692" style="zoom:50%;" />

Basically a turing machine that check termination within a certain time complexity.

<img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201104091350967.png" alt="image-20201104091350967" style="zoom:50%;" /> 

<img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201104114837390.png" alt="image-20201104114837390" style="zoom:50%;" />

A function ![f:\mathbb{N}\rightarrow\mathbb{N}](https://wikimedia.org/api/rest_v1/media/math/render/svg/be3309c065fe3d95db35e4fe45b5a43746e06cdf) is time-constructible if there exists a deterministic Turing machine such that for every ![n\in \mathbb {N} ](https://wikimedia.org/api/rest_v1/media/math/render/svg/d059936e77a2d707e9ee0a1d9575a1d693ce5d0b), if the machine is started with an input of *n* ones, it will halt after precisely *f*(*n*) steps.  All polynomials with non-negative integer coefficients are time-constructible, as are exponential functions such as 2*n*.

It is shown that If f(n) is fast-growing (say >4n) then we can decide HALTING_f in time O(f(n))^3 using a UTM as described above. 

<img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201104162736180.png" alt="image-20201104162736180" style="zoom:50%;" />

<img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201104172628591.png" alt="image-20201104172628591" style="zoom:50%;" />

## Linear Programming

Linear programming is an optimization problem that generally means to maximize a objective functions given a set of constraints, e.g.:

![image-20201108145114568](C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201108145114568.png)

### Lemma 26.3

The slack form of a LP is uniquely determined by the set of free variables.

## Lecture 7 Try to be logical

### SAT

<img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201111092137339.png" alt="image-20201111092137339" style="zoom:50%;" />

  <img src="C:\Users\weilu\AppData\Roaming\Typora\typora-user-images\image-20201111092727658.png" alt="image-20201111092727658" style="zoom:50%;" />

- literal: $p$ or $\neg p$, where p is a letter
- clause is a $l_1 \lor \dots \lor l_n$, empty conjunction is $\bot$.
  - unit clause: clause with just one literal
- if $l$ is literal then $\bar{l}$ is the opposite literal.
- satisfiable if there is an assignment such that $\theta(\gamma)=T$ for all $\gamma \in \Gamma$

<img src="D:\UOM\Projects\Notes\in progress\COMP36111.assets\image-20201111093417191.png" alt="image-20201111093417191" style="zoom:50%;" />

obviously, since propositional SAT is in NPTime, then these two problems are in NPTime as well.

<img src="D:\UOM\Projects\Notes\in progress\COMP36111.assets\image-20201111093643106.png" alt="image-20201111093643106" style="zoom:50%;" />

- resolve takes in a set of clauses and a literal and assumed the literal is true.
  - the first if statements remove $\gamma$ (clauses) that is already true
  - the second if statements remove unsatisfiable literals from each clause
  - so we will left with the modified clauses such that if it is satisfiable, then the given set of clauses with the assumption that $l$ is true is satisfiable.
  - in short it removes the redundant clauses/literals given $l$ is true.

### Horn

<img src="D:\UOM\Projects\Notes\in progress\COMP36111.assets\image-20201111112353148.png" alt="image-20201111112353148" style="zoom:50%;" />

<img src="D:\UOM\Projects\Notes\in progress\COMP36111.assets\image-20201111112602144.png" alt="image-20201111112602144" style="zoom:50%;" />

- if there is at least one negated literal in every clause, then we can simply set all variable to false to find a solution.
  - horn -> every clause at most 1 positive
  - no unit clause -> every clause has at least 2 literal
  - above two requirements guaranteed every clause has at least 1 negated literal.

### 2-SAT (Krom)

<img src="D:\UOM\Projects\Notes\in progress\COMP36111.assets\image-20201111113315599.png" alt="image-20201111113315599" style="zoom:50%;" />



<img src="D:\UOM\Projects\Notes\in progress\COMP36111.assets\image-20201111161359958.png" alt="image-20201111161359958" style="zoom: 50%;" />



<img src="D:\UOM\Projects\Notes\in progress\COMP36111.assets\image-20201111113425884.png" alt="image-20201111113425884" style="zoom:50%;" />

<img src="D:\UOM\Projects\Notes\in progress\COMP36111.assets\image-20201111113516679.png" alt="image-20201111113516679" style="zoom:50%;" />

We can represent the set of clauses (2-SAT) as directed graphs

## Lecture 8 How hard can this be

SAT is not essentially harder than 3-SAT.

####  Reducible (Many-one logspace)

<img src="D:\UOM\Projects\Notes\in progress\COMP36111.assets\image-20201125102337856.png" alt="image-20201125102337856" style="zoom:33%;" />

SAT is reducible to 3-SAT

<img src="D:\UOM\Projects\Notes\in progress\COMP36111.assets\image-20201125102744778.png" alt="image-20201125102744778" style="zoom:33%;" />

closed: if P1 is log-space reducible to P2 and P2 is in any of these classes then P1 is also in these classes.

<img src="D:\UOM\Projects\Notes\in progress\COMP36111.assets\image-20201125103111757.png" alt="image-20201125103111757" style="zoom:33%;" />

if F1 and F2 are computable in log-space, then so is F3: F2(F1()).

<img src="D:\UOM\Projects\Notes\in progress\COMP36111.assets\image-20201125103354065.png" alt="image-20201125103354065" style="zoom:33%;" />

Note that the output of f(x) from x may not be in log-space, although its working is. With that said, below is how we do it:

<img src="D:\UOM\Projects\Notes\in progress\COMP36111.assets\image-20201125103528396.png" alt="image-20201125103528396" style="zoom:33%;" />

<img src="D:\UOM\Projects\Notes\in progress\COMP36111.assets\image-20201125103623018.png" alt="image-20201125103623018" style="zoom:33%;" />

### Reducible (Many-one polytime)

<img src="D:\UOM\Projects\Notes\in progress\COMP36111.assets\image-20201125103623018.png" alt="image-20201125103623018" style="zoom:33%;" />

theoretically, log-space reducibility is more useful, and when we talk about reducibility, we assume it is log-space unless stated specifically.

We have the following definition:

<img src="D:\UOM\Projects\Notes\in progress\COMP36111.assets\image-20201125103852356.png" alt="image-20201125103852356" style="zoom: 33%;" />

  ### Cook's Theorem

<img src="D:\UOM\Projects\Notes\in progress\COMP36111.assets\image-20201125104156434.png" alt="image-20201125104156434" style="zoom:33%;" />

- **SAT is an NP problem**
  Assignment can be verified in polynomial time. We do not proof it here.

- **Every NP problem can be reduced to an instance of a SAT problem by polynomial time many-one reduction.**
  Given an NP problem (and turing machine M that solves it), for each of its input, we specify a boolean expression which is satisfiable iff the machine M accepts it (the given problem's machine). The expression uses the following variables:

  | Variables | Intended interpretation                                      | How many?    |
  | --------- | ------------------------------------------------------------ | ------------ |
  | *Ti,j,k*  | True if tape cell *i* contains symbol *j* at step *k* of the computation. | O(*p*(*n*)2) |
  | *Hi,k*    | True if the *M*'s read/write head is at tape cell *i* at step *k* of the computation. | O(*p*(*n*)2) |
  | *Qq,k*    | True if *M* is in state *q* at step *k* of the computation.  | O(*p*(*n*))  |

Here we basically have a propositional variables for each possible configuration of M, assign it true if the configuration is used in the machine M for input I, therefore we can encode any run on the machine with at most p(n) x p(n) size:

<img src="D:\UOM\Projects\Notes\in progress\COMP36111.assets\image-20201125134400916.png" alt="image-20201125134400916" style="zoom:50%;" />

<img src="D:\UOM\Projects\Notes\in progress\COMP36111.assets\image-20201125134411504.png" alt="image-20201125134411504" style="zoom:50%;" />

We then defines a bunch of propositional expressions such that for each encoding of any run on the machine M as demonstrated above, it satisfies the properties of the turing machine. In other words, it is to ensure that the encoding we have using propositional expressions above cannot have any ill-formed run of turing machine M.

<img src="D:\UOM\Projects\Notes\in progress\COMP36111.assets\image-20201125134614852.png" alt="image-20201125134614852" style="zoom: 33%;" />

<img src="D:\UOM\Projects\Notes\in progress\COMP36111.assets\image-20201125134916852.png" alt="image-20201125134916852" style="zoom:33%;" />

<img src="D:\UOM\Projects\Notes\in progress\COMP36111.assets\image-20201125140812600.png" alt="image-20201125140812600" style="zoom:33%;" />

<img src="D:\UOM\Projects\Notes\in progress\COMP36111.assets\image-20201125142613878.png" alt="image-20201125142613878" style="zoom:33%;" />

By adding these propositional expression (like some rules to the encoding), we ensure that it does not leads to any ill-formed runs (e.g. impossible transitions or head appear in two places at the same time). The non-deterministic property mentioned basically to note that if you are in a particular state, then you may do several transitions from that point.

If there is an accepting computation for *M* on input *I*, then *B* is satisfiable by assigning *T**i,j,k*, *H**i,k* and *Q**i,k* their intended interpretations. On the other hand, if *B* is satisfiable, then there is an accepting computation for *M* on input *I* that follows the steps indicated by the assignments to the variables.

All these clauses form a resulting set of clauses $\Gamma_x$, and this set of clauses is satisfiable iff M accepts x.

There are *O*(*p*(*n*)2) Boolean variables, each encode-able in space *O*(log *p*(*n*)). The number of clauses is *O*(*p*(*n*)3) so the size of *B* is *O*(log(*p*(*n*))*p*(*n*)3). Thus the transformation is certainly a polynomial-time many-one reduction, as required, and actually if we check it, it is in log-space as well, so:

<img src="D:\UOM\Projects\Notes\in progress\COMP36111.assets\image-20201125152824977.png" alt="image-20201125152824977" style="zoom: 50%;" />

Next we show that 3-SAT is NPTime-complete.

- Since SAT is NP-hard, then obviously 3-SAT is NP-hard
- And we already shown that SAT can be reduce to 3-SAT in log-space, and every NP problem can reduce to SAT in log-space, so every NP problem can be reduce to 3-SAT as well.

<img src="D:\UOM\Projects\Notes\in progress\COMP36111.assets\image-20201125125146264.png" alt="image-20201125125146264" style="zoom:50%;" />

Next lets show that horn-SAT is PTime-complete:

<img src="D:\UOM\Projects\Notes\in progress\COMP36111.assets\image-20201125153758532.png" alt="image-20201125153758532" style="zoom: 50%;" />

deterministic means there is only one possible transition that is true, so it means there is at most one positive literals for transition above (q,h,p are negated, and there is only one positive p).

<img src="D:\UOM\Projects\Notes\in progress\COMP36111.assets\image-20201125153954494.png" alt="image-20201125153954494" style="zoom:50%;" />

### Space

#### st-Con is NLogSpace-complete

<img src="D:\UOM\Projects\Notes\in progress\COMP36111.assets\image-20201125154228449.png" alt="image-20201125154228449" style="zoom:33%;" />

basically you just construct a graph from initial configuration to the final accepting configuration of the given turing machine. Since the original turing machine is log-space, the construction of the graph is log-space as well, therefore it is nlogspace reducible. And we already shown before st-con is nlogspace-hard,  therefore it is nlogspace-complete.

#### QBF is PSpace-complete

We shown previously QBF is PSpace

<img src="D:\UOM\Projects\Notes\in progress\COMP36111.assets\image-20201125161934651.png" alt="image-20201125161934651" style="zoom: 33%;" />

if some L is PSpace, then its configuration graph can at most have $c^{f(n)\log f(n)}$ vertices, because it is the size of the tape is $f(n)$ and $\log f(n)$ is the size of the heads. We can represents each configuration as strings of bits, then strings of bits can be encoded into propositional variables, e.g. 010 = $\neg a\and b\and\neg c$, and we call this sequence of propositional letters $\bar{p}$ and its length is of-course in polynomial:

<img src="D:\UOM\Projects\Notes\in progress\COMP36111.assets\image-20201125162714940.png" alt="image-20201125162714940" style="zoom:33%;" />

Now we can encode configurations in polynomial size, then we can encode transition t representing as: $\psi(\bar{p},\bar{q})$. Then if we can show that given any two configurations (p,q) and i steps (where $1\le i\le \log(c^{f'(n)})=f'(n)\cdot \log(c)$), $\psi_i(\bar{p},\bar{q})$ is reachable within $2^{i}$ step, then we can establish the proof.

<img src="D:\UOM\Projects\Notes\in progress\COMP36111.assets\image-20201125164205656.png" alt="image-20201125164205656" style="zoom: 33%;" />

Our first thought is too big because it doubles in size every time i+1. (? arent we proofing $2^i$ so it is fine?). The trick that we use only has a single $\psi_i$ so it does not double in length, but effectively they mean the same thing, we rephrase it such that it only increase by some constant amount as we do not double the $\psi_i$ (?? why this works).

So now we have size of configuration is in exp, but we only need log of it to reach between any two configurations, and log of exp is in polynomial, so it is in polynomial space.

## Lecture 9 Two NP-Complete Problems

### 3-colorability

<img src="D:\UOM\Projects\Notes\in progress\COMP36111.assets\image-20201123094549618.png" alt="image-20201123094549618" style="zoom:50%;" />

<img src="D:\UOM\Projects\Notes\in progress\COMP36111.assets\image-20201123094637751.png" alt="image-20201123094637751" style="zoom:50%;" />

We are going to show it is NPTime-hard, therefore showing it is NP-complete. We show this from 3SAT: from a set of 3 literals $\Gamma$, we compute (in log-space) a graph $G_{\Gamma}$ and show that they are iff satisfiable.

- dash line means they have the same color

<img src="D:\UOM\Projects\Notes\in progress\COMP36111.assets\image-20201123095615166.png" alt="image-20201123095615166" style="zoom:33%;" />

For asserting a=b

<img src="D:\UOM\Projects\Notes\in progress\COMP36111.assets\image-20201123095626113.png" alt="image-20201123095626113" style="zoom:33%;" />

For asserting a=b xor c

<img src="D:\UOM\Projects\Notes\in progress\COMP36111.assets\image-20201123103157998.png" alt="image-20201123103157998" style="zoom:33%;" />

For asserting a=b or c

<img src="D:\UOM\Projects\Notes\in progress\COMP36111.assets\image-20201123105503889.png" alt="image-20201123105503889" style="zoom:33%;" />

For asserting a=b or c or d, this can used to means either any of the variable in the clause

<img src="D:\UOM\Projects\Notes\in progress\COMP36111.assets\image-20201123105827172.png" alt="image-20201123105827172" style="zoom: 80%;" />

 